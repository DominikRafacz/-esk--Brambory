<?xml version="1.0" encoding="UTF-8" standalone="yes"?><template><description></description><name>České Brambory</name><snippet><connections><id>3240a739-aa3d-48c8-8ac9-31d06b0e1f1b</id><parentGroupId>99a69080-905c-4510-9a9b-158e7300959b</parentGroupId><backPressureDataSizeThreshold>0 MB</backPressureDataSizeThreshold><backPressureObjectThreshold>0</backPressureObjectThreshold><destination><groupId>99a69080-905c-4510-9a9b-158e7300959b</groupId><id>7c63b6b5-b3c0-49c3-a37e-b36098a91b8a</id><type>PROCESSOR</type></destination><flowFileExpiration>0 sec</flowFileExpiration><labelIndex>1</labelIndex><name></name><selectedRelationships>success</selectedRelationships><source><groupId>99a69080-905c-4510-9a9b-158e7300959b</groupId><id>5d9ae9b1-4962-46e5-ae1b-8319fe82a2ef</id><type>PROCESSOR</type></source><zIndex>0</zIndex></connections><processors><id>7c63b6b5-b3c0-49c3-a37e-b36098a91b8a</id><parentGroupId>99a69080-905c-4510-9a9b-158e7300959b</parentGroupId><position><x>477.49188232421875</x><y>601.3852844238281</y></position><config><bulletinLevel>WARN</bulletinLevel><comments></comments><concurrentlySchedulableTaskCount>1</concurrentlySchedulableTaskCount><defaultConcurrentTasks><entry><key>TIMER_DRIVEN</key><value>1</value></entry><entry><key>EVENT_DRIVEN</key><value>0</value></entry><entry><key>CRON_DRIVEN</key><value>1</value></entry></defaultConcurrentTasks><defaultSchedulingPeriod><entry><key>TIMER_DRIVEN</key><value>0 sec</value></entry><entry><key>CRON_DRIVEN</key><value>* * * * * ?</value></entry></defaultSchedulingPeriod><descriptors><entry><key>Hadoop Configuration Resources</key><value><description>A file or comma separated list of files which contains the Hadoop file system configuration. Without this, Hadoop will search the classpath for a 'core-site.xml' and 'hdfs-site.xml' file or will revert to a default configuration.</description><displayName>Hadoop Configuration Resources</displayName><dynamic>false</dynamic><name>Hadoop Configuration Resources</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Kerberos Principal</key><value><description>Kerberos principal to authenticate as. Requires nifi.kerberos.krb5.file to be set in your nifi.properties</description><displayName>Kerberos Principal</displayName><dynamic>false</dynamic><name>Kerberos Principal</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Kerberos Keytab</key><value><description>Kerberos keytab associated with the principal. Requires nifi.kerberos.krb5.file to be set in your nifi.properties</description><displayName>Kerberos Keytab</displayName><dynamic>false</dynamic><name>Kerberos Keytab</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Kerberos Relogin Period</key><value><defaultValue>4 hours</defaultValue><description>Period of time which should pass before attempting a kerberos relogin</description><displayName>Kerberos Relogin Period</displayName><dynamic>false</dynamic><name>Kerberos Relogin Period</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Directory</key><value><description>The parent HDFS directory to which files should be written</description><displayName>Directory</displayName><dynamic>false</dynamic><name>Directory</name><required>true</required><sensitive>false</sensitive><supportsEl>true</supportsEl></value></entry><entry><key>Conflict Resolution Strategy</key><value><allowableValues><displayName>replace</displayName><value>replace</value></allowableValues><allowableValues><displayName>ignore</displayName><value>ignore</value></allowableValues><allowableValues><displayName>fail</displayName><value>fail</value></allowableValues><defaultValue>fail</defaultValue><description>Indicates what should happen when a file with the same name already exists in the output directory</description><displayName>Conflict Resolution Strategy</displayName><dynamic>false</dynamic><name>Conflict Resolution Strategy</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Block Size</key><value><description>Size of each block as written to HDFS. This overrides the Hadoop Configuration</description><displayName>Block Size</displayName><dynamic>false</dynamic><name>Block Size</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>IO Buffer Size</key><value><description>Amount of memory to use to buffer file contents during IO. This overrides the Hadoop Configuration</description><displayName>IO Buffer Size</displayName><dynamic>false</dynamic><name>IO Buffer Size</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Replication</key><value><description>Number of times that HDFS will replicate each file. This overrides the Hadoop Configuration</description><displayName>Replication</displayName><dynamic>false</dynamic><name>Replication</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Permissions umask</key><value><description>A umask represented as an octal number which determines the permissions of files written to HDFS. This overrides the Hadoop Configuration dfs.umaskmode</description><displayName>Permissions umask</displayName><dynamic>false</dynamic><name>Permissions umask</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Remote Owner</key><value><description>Changes the owner of the HDFS file to this value after it is written. This only works if NiFi is running as a user that has HDFS super user privilege to change owner</description><displayName>Remote Owner</displayName><dynamic>false</dynamic><name>Remote Owner</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Remote Group</key><value><description>Changes the group of the HDFS file to this value after it is written. This only works if NiFi is running as a user that has HDFS super user privilege to change group</description><displayName>Remote Group</displayName><dynamic>false</dynamic><name>Remote Group</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Compression codec</key><value><allowableValues><displayName>NONE</displayName><value>NONE</value></allowableValues><allowableValues><displayName>DEFAULT</displayName><value>DEFAULT</value></allowableValues><allowableValues><displayName>BZIP</displayName><value>BZIP</value></allowableValues><allowableValues><displayName>GZIP</displayName><value>GZIP</value></allowableValues><allowableValues><displayName>LZ4</displayName><value>LZ4</value></allowableValues><allowableValues><displayName>SNAPPY</displayName><value>SNAPPY</value></allowableValues><allowableValues><displayName>AUTOMATIC</displayName><value>AUTOMATIC</value></allowableValues><defaultValue>NONE</defaultValue><description></description><displayName>Compression codec</displayName><dynamic>false</dynamic><name>Compression codec</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry></descriptors><lossTolerant>false</lossTolerant><penaltyDuration>30 sec</penaltyDuration><properties><entry><key>Hadoop Configuration Resources</key><value>/etc/hadoop/2.4.0.0-169/0/hdfs-site.xml,/etc/hadoop/2.4.0.0-169/0/core-site.xml</value></entry><entry><key>Kerberos Principal</key></entry><entry><key>Kerberos Keytab</key></entry><entry><key>Kerberos Relogin Period</key></entry><entry><key>Directory</key><value>/user/brambrory</value></entry><entry><key>Conflict Resolution Strategy</key></entry><entry><key>Block Size</key></entry><entry><key>IO Buffer Size</key></entry><entry><key>Replication</key></entry><entry><key>Permissions umask</key></entry><entry><key>Remote Owner</key></entry><entry><key>Remote Group</key></entry><entry><key>Compression codec</key></entry></properties><runDurationMillis>0</runDurationMillis><schedulingPeriod>0 sec</schedulingPeriod><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><yieldDuration>1 sec</yieldDuration></config><name>PutDetectorMetadataToHDFS</name><relationships><autoTerminate>true</autoTerminate><description>Files that could not be written to HDFS for some reason are transferred to this relationship</description><name>failure</name></relationships><relationships><autoTerminate>true</autoTerminate><description>Files that have been successfully written to HDFS are transferred to this relationship</description><name>success</name></relationships><state>RUNNING</state><style/><supportsEventDriven>false</supportsEventDriven><supportsParallelProcessing>true</supportsParallelProcessing><type>org.apache.nifi.processors.hadoop.PutHDFS</type></processors><processors><id>5d9ae9b1-4962-46e5-ae1b-8319fe82a2ef</id><parentGroupId>99a69080-905c-4510-9a9b-158e7300959b</parentGroupId><position><x>-126.0</x><y>488.0</y></position><config><bulletinLevel>WARN</bulletinLevel><comments></comments><concurrentlySchedulableTaskCount>1</concurrentlySchedulableTaskCount><defaultConcurrentTasks><entry><key>TIMER_DRIVEN</key><value>1</value></entry><entry><key>EVENT_DRIVEN</key><value>0</value></entry><entry><key>CRON_DRIVEN</key><value>1</value></entry></defaultConcurrentTasks><defaultSchedulingPeriod><entry><key>TIMER_DRIVEN</key><value>0 sec</value></entry><entry><key>CRON_DRIVEN</key><value>* * * * * ?</value></entry></defaultSchedulingPeriod><descriptors><entry><key>Input Directory</key><value><description>The input directory from which to pull files</description><displayName>Input Directory</displayName><dynamic>false</dynamic><name>Input Directory</name><required>true</required><sensitive>false</sensitive><supportsEl>true</supportsEl></value></entry><entry><key>File Filter</key><value><defaultValue>[^\.].*</defaultValue><description>Only files whose names match the given regular expression will be picked up</description><displayName>File Filter</displayName><dynamic>false</dynamic><name>File Filter</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Path Filter</key><value><description>When Recurse Subdirectories is true, then only subdirectories whose path matches the given regular expression will be scanned</description><displayName>Path Filter</displayName><dynamic>false</dynamic><name>Path Filter</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Batch Size</key><value><defaultValue>10</defaultValue><description>The maximum number of files to pull in each iteration</description><displayName>Batch Size</displayName><dynamic>false</dynamic><name>Batch Size</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Keep Source File</key><value><allowableValues><displayName>true</displayName><value>true</value></allowableValues><allowableValues><displayName>false</displayName><value>false</value></allowableValues><defaultValue>false</defaultValue><description>If true, the file is not deleted after it has been copied to the Content Repository; this causes the file to be picked up continually and is useful for testing purposes.  If not keeping original NiFi will need write permissions on the directory it is pulling from otherwise it will ignore the file.</description><displayName>Keep Source File</displayName><dynamic>false</dynamic><name>Keep Source File</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Recurse Subdirectories</key><value><allowableValues><displayName>true</displayName><value>true</value></allowableValues><allowableValues><displayName>false</displayName><value>false</value></allowableValues><defaultValue>true</defaultValue><description>Indicates whether or not to pull files from subdirectories</description><displayName>Recurse Subdirectories</displayName><dynamic>false</dynamic><name>Recurse Subdirectories</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Polling Interval</key><value><defaultValue>0 sec</defaultValue><description>Indicates how long to wait before performing a directory listing</description><displayName>Polling Interval</displayName><dynamic>false</dynamic><name>Polling Interval</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Ignore Hidden Files</key><value><allowableValues><displayName>true</displayName><value>true</value></allowableValues><allowableValues><displayName>false</displayName><value>false</value></allowableValues><defaultValue>true</defaultValue><description>Indicates whether or not hidden files should be ignored</description><displayName>Ignore Hidden Files</displayName><dynamic>false</dynamic><name>Ignore Hidden Files</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Minimum File Age</key><value><defaultValue>0 sec</defaultValue><description>The minimum age that a file must be in order to be pulled; any file younger than this amount of time (according to last modification date) will be ignored</description><displayName>Minimum File Age</displayName><dynamic>false</dynamic><name>Minimum File Age</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Maximum File Age</key><value><description>The maximum age that a file must be in order to be pulled; any file older than this amount of time (according to last modification date) will be ignored</description><displayName>Maximum File Age</displayName><dynamic>false</dynamic><name>Maximum File Age</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Minimum File Size</key><value><defaultValue>0 B</defaultValue><description>The minimum size that a file must be in order to be pulled</description><displayName>Minimum File Size</displayName><dynamic>false</dynamic><name>Minimum File Size</name><required>true</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry><entry><key>Maximum File Size</key><value><description>The maximum size that a file can be in order to be pulled</description><displayName>Maximum File Size</displayName><dynamic>false</dynamic><name>Maximum File Size</name><required>false</required><sensitive>false</sensitive><supportsEl>false</supportsEl></value></entry></descriptors><lossTolerant>false</lossTolerant><penaltyDuration>30 sec</penaltyDuration><properties><entry><key>Input Directory</key><value>/home/nifi/detector_metadata</value></entry><entry><key>File Filter</key></entry><entry><key>Path Filter</key></entry><entry><key>Batch Size</key></entry><entry><key>Keep Source File</key></entry><entry><key>Recurse Subdirectories</key></entry><entry><key>Polling Interval</key></entry><entry><key>Ignore Hidden Files</key></entry><entry><key>Minimum File Age</key></entry><entry><key>Maximum File Age</key></entry><entry><key>Minimum File Size</key></entry><entry><key>Maximum File Size</key></entry></properties><runDurationMillis>0</runDurationMillis><schedulingPeriod>0 sec</schedulingPeriod><schedulingStrategy>TIMER_DRIVEN</schedulingStrategy><yieldDuration>1 sec</yieldDuration></config><name>GetDetectorMetadataFile</name><relationships><autoTerminate>false</autoTerminate><description>All files are routed to success</description><name>success</name></relationships><state>RUNNING</state><style/><supportsEventDriven>false</supportsEventDriven><supportsParallelProcessing>true</supportsParallelProcessing><type>org.apache.nifi.processors.standard.GetFile</type></processors></snippet><timestamp>01/03/2021 10:41:11 UTC</timestamp></template>